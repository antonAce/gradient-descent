{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f61778a6",
   "metadata": {},
   "source": [
    "# Overview of the existing stochastic gradient descent algorithms\n",
    "\n",
    "## Numerical derivatives approximation\n",
    "\n",
    "Aside from the general definition of the optimization problem, we consider it as a smooth problem of stochastic programming (1) [1], which we express in the following equation:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\min_{x \\in X} f(x) = \\mathbb{E}F(x, \\xi) = \\int_{\\xi \\in \\Xi} F(x, \\xi) P(d \\xi), X \\subseteq \\mathbb{R}^{n} & (1)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "We define the target function $ F(x) $ as a continuous and smooth (differentiable) function, $ x \\in \\mathbb{R} $ - a dependent variable defined on a space $ \\inf_{x \\in X} F(x) > - \\infty $, $ \\xi $ - a random variable modeled by a previously unknown probability distribution $ P(d \\xi) $ on the event space $ \\Xi $. By definition, the investigated function $ f $ is a non-analytical and non-smooth function representing an averaged value of a particular stochastic oracle $ \\tilde{F}(x, \\xi_{i}) $, which also has an approximated value of the gradient $ \\nabla \\tilde{F}(x, \\xi_{i}) $. A well-known method for estimating the function $ f $ is modeling by the Monte Carlo method or any other stochastic method with a predetermined number of experiments, where $ \\xi $ is a random variable in the mathematical functional model $ \\tilde{F}(x, \\xi_{i}) $ and the formula for the gradient $ \\nabla \\tilde{F}(x, \\xi_{i}) $.\n",
    "\n",
    "The method of gradient descent in the problem of stochastic optimization can also be considered an iterative process:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "x_{i+1} = \\Pi_{X}(x_{i} - \\lambda_{i} H_{i}^{-1} g(x_{i}, M(x_{i}))) & (2)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "Here the parameter $ \\lambda_{i} $ is a step of the gradient descent method, $ H_{i} $ - the Hesse matrix approximation for the optimization function at each step of the method $ i $, and $ g(x_{i}, M(x_{i})) $ is an approximation of the gradient $ \\nabla \\tilde{F}(x, \\xi_{i}) $ for a specific sample from the set of measurements $ M(x_{i}) $. Here the projection operator $ \\Pi_{X}(y) $ is given by the formula:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\Pi_{X}(y) = \\arg \\inf_{x \\in X} \\{ || y - x || \\} & (3)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "Before a detailed analysis of stochastic gradient algorithms, we define the gradient of the function and the gradient descent. The gradient of the function $ F $ in space $ \\mathbb{R}^{n} $ is a column vector of partial derivatives with respect to each variable $ x_1, ..., x_n $ for a certain point $ a \\in \\mathbb{R}^{n} $:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\nabla F(a) = \\begin{bmatrix} \\frac{\\partial F(a)}{\\partial x_{1}}, \\cdots, \\frac{\\partial F(a)}{\\partial x_{n}} \\end{bmatrix} & (4)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "The main feature of the gradient is the indication of the direction of the largest increment of the function $ F $ at a given point. The main iteration scheme for gradient descent methods is based on it: by assumption, the point of a minimum of the target function is in the direction opposite to the gradient, i.e. the anti-gradient or $ - \\nabla F $. The iteration scheme of gradient descent methods consists of successive updating of independent parameters of the model $ \\theta_{i} $ by the value of the anti-gradient with a certain constant step $ \\lambda \\in [0, 1] $: $ \\theta_{i+1} = \\theta_{i} - \\lambda \\cdot \\nabla F $.\n",
    "\n",
    "The work of the algorithm continues until one of the set limit conditions is reached:\n",
    " - a finite number of iterations;\n",
    " - by the theorem on the necessary condition for the existence of a minimum: $ || \\nabla F(x_{k}) || \\to 0 $ or in the case of stochastic approximation $ \\mathbb{E} || \\nabla F(x_{k}) || \\to 0 $.\n",
    "\n",
    "In this paper, the value of the gradient is calculated by the finite difference method: the value of the derivative is a result of a decomposing transformation of the function into a Taylor series:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "f(x + h) = \\sum_{i=1}^{\\infty} \\frac{f^{(i)}(x)}{i!} h^i & (5)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "Before defining the concept of Taylor series approximation and Taylor's Theorem, let us show the basic properties of a function that is $N$-times differentiable.\n",
    "\n",
    "***General Rolle’s theorem [2].*** Given the analytical function $ F: (a, b) \\to \\mathbb{R} $ defined on the segment $ (a, b) $ where $ a, b \\in \\mathbb{R}, a < b $. For any natural number $ n \\in \\mathbb{N} $ function is $ N $-times differentiable on the open segment $ (a, b) $ and derivatives $ F, F', ..., F^{(n-1)} $ of the function are continuous on the closed interval $ [a, b] $. Then, if the condition is satisfied $ F(a) = F'(a) = ... = F^{(n-1)}(a) = F(b) = 0 $, on the defined segment exists an element $ c \\in (a, b) $ that will satisfy the following condition $ F^{n}(c) = 0 $. Let us use the properties from the general theorem of Rolle’s to construct an $ N $-degree polynomial for the function $ F $, satisfying the conditions described above, and formulating the following theorem.\n",
    "\n",
    "***Taylor’s theorem [3].*** Given the analytical function $ F: (a, b) \\to \\mathbb{R} $ defined on the segment $ (a, b) $ where $ a, b \\in \\mathbb{R}, a < b $. For any natural number $ n \\in \\mathbb{N} $ function is $ N $-times differentiable on the open segment $ (a, b) $ and derivatives $ F, F', ..., F^{(n - 1)} $ of the function are continuous on the closed interval $ [a, b] $. Then exists an element $ c \\in (a, b) $, that guarantees the existence of a single Taylor’s series decomposition:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "f(b) = \\sum_{k=0}^{n-1}\\frac{f^{(k)}(a)}{k!} (b - a)^{k} + \\frac{f^{(n)}(c)}{n!}(b - a)^{n} & (6)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "This theorem can be interpreted otherwise: any analytical $ N $-times differentiable function $ F $ can be “reconstructed” at any point of a $ \\mathbb{R} $ space, if we know the value of the function and value of any order derivatives for a single element $ x_{0} \\in \\mathbb{R} $. The equation of Taylor’s series polynomial can be simplified by discarding unnecessary residual terms that have higher accuracy order than the first order. By substituting the difference between of anchor element $ x_{0} \\in \\mathbb{R} $ and other elements from $ \\mathbb{R} $ space with $ h = x - x_{0} $:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "f(x_{0} + h) = f(x_{0}) + f'(x_{0})h + o(h) & (7)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "By replacing the terms of the equation we get the numerical formula of the first-order derivative of the function $ f $:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "f_{x_{0} + 0}'(x_{0}) = \\frac{f(x_{0} + h) - f(x_{0})}{h} + o(h) & (8)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "Considering equations (6) and (7) we can get numerical derivative for a negative direction $ f_{x_{0} - 0}'(x_{0}) $ by constructing Taylor’s series with the negative value of the step difference $ h < 0 $:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "f(x_{0} - h) = f(x_{0}) + f'(x_{0})(-h) + o(h) \\implies f_{x_{0} - 0}'(x_{0}) = \\frac{f(x_{0}) - f(x_{0} - h)}{h} + o(h) & (9)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "By discarding residual terms after the first term, we get the finite-difference approximation of for the derivative value of $ f $ with the precision order $ O(h) $:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\begin{cases}\n",
    "f'(x) = \\frac{f(x + h) - f(x)}{h} - \\frac{h f''(\\xi)}{2} \\\\\n",
    "f'(x) = \\frac{f(x) - f(x - h)}{h} + \\frac{h f''(\\xi)}{2}\n",
    "\\end{cases} & (10)\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "368b70eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "def grad_left(F: Callable[[np.array], np.array], x: np.array, h=0.001) -> np.array:\n",
    "    \"\"\"A finite-difference approximation for left-side gradient $\\nabla F_{-}(x)$ with the precision order $O(h)$.\n",
    "\n",
    "    Args:\n",
    "        F (Callable[[np.array], np.array]): a target function $F(x)$ with a single input argument $x \\in \\mathbb{R}^n$.\n",
    "        x (np.array): an input vector $x \\in \\mathbb{R}^n$, where the derivative is calculated.\n",
    "        h (float, optional): a step of the derivative partitioning grid with the range of $0<h<1$. The lower value, the higher gradient precision. Defaults to 0.001.\n",
    "\n",
    "    Returns:\n",
    "        np.array: a gradient vector approximation $\\nabla F_{-}(x)$.\n",
    "    \"\"\"\n",
    "\n",
    "    n, grad = len(x), np.zeros(x.shape)\n",
    "    \n",
    "    for i in range(n):\n",
    "        vh = h * np.eye(1, n, i).reshape((n, ))\n",
    "        grad[i] = (F(x) - F(x - vh)) / h\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "def grad_right(F: Callable[[np.array], np.array], x: np.array, h=0.001) -> np.array:\n",
    "    \"\"\"A finite-difference approximation for right-side gradient $\\nabla F_{+}(x)$ with the precision order $O(h)$.\n",
    "\n",
    "    Args:\n",
    "        F (Callable[[np.array], np.array]): a target function $F(x)$ with a single input argument $x \\in \\mathbb{R}^n$.\n",
    "        x (np.array): an input vector $x \\in \\mathbb{R}^n$, where the derivative is calculated.\n",
    "        h (float, optional): a step of the derivative partitioning grid with the range of $0<h<1$. The lower value, the higher gradient precision. Defaults to 0.001.\n",
    "\n",
    "    Returns:\n",
    "        np.array: a gradient vector approximation $\\nabla F_{+}(x)$.\n",
    "    \"\"\"\n",
    "\n",
    "    n, grad = len(x), np.zeros(x.shape)\n",
    "\n",
    "    for i in range(n):\n",
    "        vh = h * np.eye(1, n, i).reshape((n, ))\n",
    "        grad[i] = (F(x + vh) - F(x)) / h\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046937f3",
   "metadata": {},
   "source": [
    "Let us experimentally evaluate the accuracy of gradient implementations on the set of test functions:\n",
    "\n",
    " - $ f(x) = x^3 + 2x^2 + 12x + 100 \\implies f'(x) = 3x^2 + 4x + 12, f'(2.0) = 32.0 $\n",
    " - $ F(x, y) = x^2 + xy + y^2 \\implies \\begin{cases} \\frac{\\partial F(x, y)}{\\partial x} = 2x + y, \\frac{\\partial F(2.0, -1.0)}{\\partial x} = 3.0 \\\\ \\frac{\\partial F(x, y)}{\\partial y} = x + 2y, \\frac{\\partial F(2.0, -1.0)}{\\partial y} = 0.0\n",
    " \\end{cases} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "489ebf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 1e-6\n",
    "x_onedim = np.array([2.0])\n",
    "x_muldim = np.array([2.0, -1.0])\n",
    "\n",
    "f = lambda x: x ** 3 + 2 * x ** 2 + 12 * x + 100\n",
    "F = lambda x: x[0] ** 2 + x[0] * x[1] + x[1] ** 2\n",
    "\n",
    "np.testing.assert_almost_equal(grad_left(f, x_onedim, h=h), 32.0, decimal=5)\n",
    "np.testing.assert_almost_equal(grad_right(f, x_onedim, h=h), 32.0, decimal=5)\n",
    "\n",
    "np.testing.assert_almost_equal(grad_left(F, x_muldim, h=h), np.array([3.0, 0.0]), decimal=5)\n",
    "np.testing.assert_almost_equal(grad_right(F, x_muldim, h=h), np.array([3.0, 0.0]), decimal=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4144b1b",
   "metadata": {},
   "source": [
    "The accuracy of the approximation depends on the number of nodes on the numerical partitioning grid, thus the smaller step difference, the higher precision. Using the Runge-Romberg-Richardson algorithm [4] we can achieve an increase in the order of the precision of the partitioning grid up to $ O(h^2) $ without adding extra iterations to the approximation algorithm. The approach is based on empirical estimation of the target function value $ z $ while decreasing the step size $ h $. The estimation is based on the assumption:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "z \\approx \\zeta(h) + O(h^p) & (11)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "Let us define a new partitioning grid with the step value of $ r \\cdot h $ and then calculate the value of a target function $ z $ on the grid:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "z \\approx \\zeta(r \\cdot h) + r^p \\cdot O(h^p) & (12)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "By combining two partition grids from (11) and (12), we rearrange terms and discard the main term of the precision order $ O(h^p) $ and get an estimation of the target function value $ z $ with the higher precision order:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "z \\approx \\frac{r^p \\zeta(h) - \\zeta(r \\cdot h)}{r^p - 1} + O(h^{p+1}) & (13)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "Now if we can derive an estimation for the definition in (11) by extracting from it the equation (13):\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "O(h^p) \\approx \\frac{\\zeta(h) - \\zeta(r \\cdot h)}{r^p - 1} & (14)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "By applying the estimations (13) - (14) we get the finite-difference approximation equations for left-side, central and right-side derivatives with the precision order $ O(h^2) $:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\begin{cases} f'(x) = \\frac{-3f(x) + 4f(x + h) - f(x + 2h)}{2h} + \\frac{h^{2} f'''(\\xi)}{3} \\\\ f'(x) = \\frac{f(x + h) - f(x - h)}{2h} + \\frac{h^{2} f'''(\\xi)}{6} \\\\ f'(x) = \\frac{f(x - 2h) - 4f(x - h) + 3f(x)}{2h} + \\frac{h^{2} f'''(\\xi)}{3} \\end{cases} & (15)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "The approach of approximating the gradient value of the target function with a finite-difference schema lets us generalize optimization problems on any kind of analytical functions. Therefore, the convergence of the modern gradient descent algorithms is theoretically proven only for convex and smooth function types. We propose a modification to the existing gradient methods using finite-difference schema that is robust to the non-convex and non-smooth functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67ee640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_ext_left(F: Callable[[np.array], np.array], x: np.array, h=0.001) -> np.array:\n",
    "    \"\"\"A finite-difference approximation for left-side gradient $\\nabla F_{-}(x)$ with the precision order $O(h^2)$.\n",
    "\n",
    "    Args:\n",
    "        F (Callable[[np.array], np.array]): a target function $F(x)$ with a single input argument $x \\in \\mathbb{R}^n$.\n",
    "        x (np.array): an input vector $x \\in \\mathbb{R}^n$, where the derivative is calculated.\n",
    "        h (float, optional): a step of the derivative partitioning grid with the range of $0<h<1$. The lower value, the higher gradient precision. Defaults to 0.001.\n",
    "\n",
    "    Returns:\n",
    "        np.array: a gradient vector approximation $\\nabla F_{-}(x)$.\n",
    "    \"\"\"\n",
    "\n",
    "    n, grad = len(x), np.zeros(x.shape)\n",
    "    \n",
    "    for i in range(n):\n",
    "        vh = h * np.eye(1, n, i).reshape((n, ))\n",
    "        grad[i] = (-3.0 * F(x) + 4.0 * F(x + vh) - F(x + 2.0 * vh)) / (2.0 * h)\n",
    "    \n",
    "    return grad\n",
    "\n",
    "\n",
    "def grad_ext_center(F: Callable[[np.array], np.array], x: np.array, h=0.001) -> np.array:\n",
    "    \"\"\"A finite-difference approximation for central gradient $\\nabla F(x)$ with the precision order $O(h^2)$.\n",
    "\n",
    "    Args:\n",
    "        F (Callable[[np.array], np.array]): a target function $F(x)$ with a single input argument $x \\in \\mathbb{R}^n$.\n",
    "        x (np.array): an input vector $x \\in \\mathbb{R}^n$, where the derivative is calculated.\n",
    "        h (float, optional): a step of the derivative partitioning grid with the range of $0<h<1$. The lower value, the higher gradient precision. Defaults to 0.001.\n",
    "\n",
    "    Returns:\n",
    "        np.array: a gradient vector approximation $\\nabla F(x)$.\n",
    "    \"\"\"\n",
    "\n",
    "    n, grad = len(x), np.zeros(x.shape)\n",
    "    \n",
    "    for i in range(n):\n",
    "        vh = h * np.eye(1, n, i).reshape((n, ))\n",
    "        grad[i] = (F(x + vh) - F(x - vh)) / (2.0 * h)\n",
    "    \n",
    "    return grad\n",
    "\n",
    "\n",
    "def grad_ext_right(F: Callable[[np.array], np.array], x: np.array, h=0.001) -> np.array:\n",
    "    \"\"\"A finite-difference approximation for right-side gradient $\\nabla F_{+}(x)$ with the precision order $O(h^2)$.\n",
    "\n",
    "    Args:\n",
    "        F (Callable[[np.array], np.array]): a target function $F(x)$ with a single input argument $x \\in \\mathbb{R}^n$.\n",
    "        x (np.array): an input vector $x \\in \\mathbb{R}^n$, where the derivative is calculated.\n",
    "        h (float, optional): a step of the derivative partitioning grid with the range of $0<h<1$. The lower value, the higher gradient precision. Defaults to 0.001.\n",
    "\n",
    "    Returns:\n",
    "        np.array: a gradient vector approximation $\\nabla F_{+}(x)$.\n",
    "    \"\"\"\n",
    "\n",
    "    n, grad = len(x), np.zeros(x.shape)\n",
    "\n",
    "    for i in range(n):\n",
    "        vh = h * np.eye(1, n, i).reshape((n, ))\n",
    "        grad[i] = (F(x - 2.0 * vh) - 4.0 * F(x - vh) + 3.0 * F(x)) / (2.0 * h)\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fffdcb",
   "metadata": {},
   "source": [
    "Let us experimentally evaluate the accuracy of the same test functions set. We get the same accuracy order for a smaller step size $ h $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c218ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 1e-5\n",
    "x_onedim = np.array([2.0])\n",
    "x_muldim = np.array([2.0, -1.0])\n",
    "\n",
    "f = lambda x: x ** 3 + 2 * x ** 2 + 12 * x + 100\n",
    "F = lambda x: x[0] ** 2 + x[0] * x[1] + x[1] ** 2\n",
    "\n",
    "np.testing.assert_almost_equal(grad_ext_left(f, x_onedim, h=h), 32.0, decimal=5)\n",
    "np.testing.assert_almost_equal(grad_ext_center(f, x_onedim, h=h), 32.0, decimal=5)\n",
    "np.testing.assert_almost_equal(grad_ext_right(f, x_onedim, h=h), 32.0, decimal=5)\n",
    "\n",
    "np.testing.assert_almost_equal(grad_ext_left(F, x_muldim, h=h), np.array([3.0, 0.0]), decimal=5)\n",
    "np.testing.assert_almost_equal(grad_ext_center(F, x_muldim, h=h), np.array([3.0, 0.0]), decimal=5)\n",
    "np.testing.assert_almost_equal(grad_ext_right(F, x_muldim, h=h), np.array([3.0, 0.0]), decimal=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
