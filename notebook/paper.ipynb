{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51885861",
   "metadata": {},
   "source": [
    "UDC 519.688\n",
    "\n",
    "# Investigating convergence rate of stochastic finite-difference optimization methods\n",
    "\n",
    "\\\n",
    "**V. I. Norkin${}^{1}$, A. Y. Kozyriev${}^{1}$**\\\n",
    "1 - Igor Sikorsky Kyiv Polytechnic Institute, Kyiv, Ukraine (vladimir.norkin@gmail.com)\n",
    "\n",
    "${}$\n",
    "\n",
    "${}$\n",
    "\n",
    "_**Abstract.** Nowadays, stochastic gradient algorithms have become one of the most popular numerical optimization methods due to their efficiency and formulation simplicity. They are used in a variety of areas: from cost reduction problems to deep learning. The common feature of all these problems is to find the optimal values for a set of independent parameters in a mathematical model that can be described by a set of equalities and inequalities. The number of computing resources and time spent to solve the problem, the accuracy of the mathematical model, etc. depends on how effective the chosen gradient descent algorithm is. In practice, stochastic gradient algorithms only show fair results for convex and smooth functions. However, most modern optimization problems do not belong to these classes (for instance, a deep neural network with a ReLU activation function is a non-smooth optimization problem). The article proposes a new modification to the existing stochastic gradient algorithms based on an averaged functions smoothing technique and finite-difference approximations with more robustness to the non-smooth target functions.\\\n",
    "\\\n",
    "**Keywords:** gradient descent methods, optimization theory, unconstrained optimization, nonsmooth problems, stochastic gradient descent methods, adaptive gradient descent methods, finite difference methods._\n",
    "\n",
    "## Introduction\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Most modern stochastic gradient algorithms have significant problems optimizing functions with multiple minima because they are more likely to converge to the local minimum than the global one. Therefore, the research of adaptive stochastic gradient methods on applied non-convex and multi-extreme optimization problems is relevant, which also considers improving existing optimization methods and proposing modifications. The second problem in applying stochastic gradient methods is the possible functions' non-smoothness and the limitations in the optimization problem. Constrained problems can be reduced to unconditional optimization problems with precise penalty functions, but the problem becomes non-smooth. Gradient methods in non-smooth optimization problems demonstrate a slow convergence rate and low robustness to the local minima.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;The primary goal of the research is to study the outcomes of replacing stochastic gradients in adaptive gradient methods with their central finite-difference alternatives and other finite-difference approximations of increased order of the local truncation error and accumulated error. To some extent, the finite-difference approximations of the gradients smooth the problem and, therefore, even out the shallow local extremes and reduce the problem's convexity.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;We conducted experimental studies on classical and adaptive gradient methods to confirm the theoretical conclusions. Studies are based on performed gradient substitutions for finite-difference estimates to avoid \"backpropagation\" when calculating gradients.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Aside from the general definition of the optimization problem, we consider it as a smooth problem of stochastic programming (1) [1], which we express in the following equation:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\min_{x \\in X} f(x) = \\mathbb{E}F(x, \\xi) = \\int_{\\xi \\in \\Xi} F(x, \\xi) P(d \\xi), X \\subseteq \\mathbb{R}^{n} & (1)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "We define the target function $ F(x) $ as a continuous and smooth (differentiable) function, $ x \\in \\mathbb{R} $ - a dependent variable defined on a space $ \\inf_{x \\in X} F(x) > - \\infty $, $ \\xi $ - a random variable modeled by a previously unknown probability distribution $ P(d \\xi) $ on the event space $ \\Xi $. By definition, the investigated function $ f $ is a non-analytical and non-smooth function representing an averaged value of a particular stochastic oracle $ \\tilde{F}(x, \\xi_i) $, which also has an approximated value of the gradient $ \\nabla \\tilde{F}(x, \\xi_i) $. A well-known method for estimating the function $ f $ is modeling by the Monte Carlo method or any other stochastic method with a predetermined number of experiments, where $ \\xi $ is a random variable in the mathematical functional model $ \\tilde{F}(x, \\xi_i) $ and the formula for the gradient $ \\nabla \\tilde{F}(x, \\xi_i) $.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;The method of gradient descent in the problem of stochastic optimization can also be considered an iterative process:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "x_{i+1} = \\Pi_{X}(x_{i} - \\lambda_{i} H_{i}^{-1} g(x_{i}, M(x_{i}))) & (2)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "Here the parameter $ \\lambda_{i} $ is a step of the gradient descent method, $ H_{i} $ - the Hesse matrix approximation for the optimization function at each step of the method $ i $, and $ g(x_{i}, M(x_{i})) $ is an approximation of the gradient $ \\nabla \\tilde{F}(x, \\xi_{i}) $ for a specific sample from the set of measurements $ M(x_{i}) $. Here the projection operator $ \\Pi_{X}(y) $ is given by the formula:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\Pi_{X}(y) = \\arg \\inf_{x \\in X} \\{ || y - x || \\} & (3)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "Before a detailed analysis of stochastic gradient algorithms, we define the gradient of the function and the gradient descent. The gradient of the function $ F $ in space $ \\mathbb{R}^{n} $ is a column vector of partial derivatives with respect to each variable $ x_1, ..., x_n $ for a certain point $ a \\in \\mathbb{R}^{n} $:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\nabla F(a) = \\begin{bmatrix} \\frac{\\partial F(a)}{\\partial x_{1}}, \\cdots, \\frac{\\partial F(a)}{\\partial x_{n}} \\end{bmatrix} & (4)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "The main feature of the gradient is the indication of the direction of the largest increment of the function $ F $ at a given point. The main iteration scheme for gradient descent methods is based on it: by assumption, the point of a minimum of the target function is in the direction opposite to the gradient, i.e. the anti-gradient or $ - \\nabla F $. The iteration scheme of gradient descent methods consists of successive updating of independent parameters of the model $ \\theta_{i} $ by the value of the anti-gradient with a certain constant step $ \\lambda \\in [0, 1] $: $ \\theta_{i+1} = \\theta_{i} - \\lambda \\cdot \\nabla F $.\n",
    "\n",
    "The work of the algorithm continues until one of the set limit conditions is reached:\n",
    " - a finite number of iterations;\n",
    " - by the theorem on the necessary condition for the existence of a minimum: $ || \\nabla F(x_{k}) || \\to 0 $ or in the case of stochastic approximation $ \\mathbb{E} || \\nabla F(x_{k}) || \\to 0 $.\n",
    "\n",
    "In this paper, the value of the gradient is calculated by the finite difference method: the value of the derivative is a result of a decomposing transformation of the function into a Taylor series:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "f(x + h) = \\sum_{i=1}^{\\infty} \\frac{f^{(i)}(x)}{i!} h^i & (5)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "Before defining the concept of Taylor series approximation and Taylor's Theorem, let us show the basic properties of a function that is $N$-times differentiable.\n",
    "\n",
    "***General Rolle’s theorem [2].*** Given the analytical function $ F: (a, b) \\to \\mathbb{R} $ defined on the segment $ (a, b) $ where $ a, b \\in \\mathbb{R}, a < b $. For any natural number $ n \\in \\mathbb{N} $ function is $ N $-times differentiable on the open segment $ (a, b) $ and derivatives $ F, F', ..., F^{(n-1)} $ of the function are continuous on the closed interval $ [a, b] $. Then, if the condition is satisfied $ F(a) = F'(a) = ... = F^{(n-1)}(a) = F(b) = 0 $, on the defined segment exists an element $ c \\in (a, b) $ that will satisfy the following condition $ F^{n}(c) = 0 $. Let us use the properties from the general theorem of Rolle’s to construct an $ N $-degree polynomial for the function $ F $, satisfying the conditions described above, and formulating the following theorem.\n",
    "\n",
    "***Taylor’s theorem [3].*** Given the analytical function $ F: (a, b) \\to \\mathbb{R} $ defined on the segment $ (a, b) $ where $ a, b \\in \\mathbb{R}, a < b $. For any natural number $ n \\in \\mathbb{N} $ function is $ N $-times differentiable on the open segment $ (a, b) $ and derivatives $ F, F', ..., F^{(n - 1)} $ of the function are continuous on the closed interval $ [a, b] $. Then exists an element $ c \\in (a, b) $, that guarantees the existence of a single Taylor’s series decomposition:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "f(b) = \\sum_{k=0}^{n-1}\\frac{f^{(k)}(a)}{k!} (b - a)^{k} + \\frac{f^{(n)}(c)}{n!}(b - a)^{n} & (6)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "This theorem can be interpreted otherwise: any analytical $ N $-times differentiable function $ F $ can be “reconstructed” at any point of a $ \\mathbb{R} $ space, if we know the value of the function and value of any order derivatives for a single element $ x_{0} \\in \\mathbb{R} $. The equation of Taylor’s series polynomial can be simplified by discarding unnecessary residual terms that have higher accuracy order than the first order. By substituting the difference between of anchor element $ x_{0} \\in \\mathbb{R} $ and other elements from $ \\mathbb{R} $ space with $ h = x - x_{0} $:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "f(x_{0} + h) = f(x_{0}) + f'(x_{0})h + o(h) & (7)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "By replacing the terms of the equation we get the numerical formula of the first-order derivative of the function $ f $:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "f_{x_{0} + 0}'(x_{0}) = \\frac{f(x_{0} + h) - f(x_{0})}{h} + o(h) & (8)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "Considering equations (6) and (7) we can get numerical derivative for a negative direction $ f_{x_{0} - 0}'(x_{0}) $ by constructing Taylor’s series with the negative value of the step difference $ h < 0 $:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "f(x_{0} - h) = f(x_{0}) + f'(x_{0})(-h) + o(h) \\implies f_{x_{0} - 0}'(x_{0}) = \\frac{f(x_{0}) - f(x_{0} - h)}{h} + o(h) & (9)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "By discarding residual terms after the first term, we get the finite-difference approximation of for the derivative value of $ f $ with the precision order $ O(h) $:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\begin{cases}\n",
    "f'(x) = \\frac{f(x + h) - f(x)}{h} - \\frac{h f''(\\xi)}{2} \\\\\n",
    "f'(x) = \\frac{f(x) - f(x - h)}{h} + \\frac{h f''(\\xi)}{2}\n",
    "\\end{cases} & (10)\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e60e1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "def lgrad(F: Callable[[np.array], np.array], x: np.array, h=0.001) -> np.array:\n",
    "    n, grad = len(x), np.zeros(x.shape)\n",
    "    \n",
    "    for i in range(n):\n",
    "        vh = h * np.eye(1, n, i).reshape((n, ))\n",
    "        grad[i] = (F(x) - F(x - vh)) / h\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "def rgrad(F: Callable[[np.array], np.array], x: np.array, h=0.001) -> np.array:\n",
    "    n, grad = len(x), np.zeros(x.shape)\n",
    "\n",
    "    for i in range(n):\n",
    "        vh = h * np.eye(1, n, i).reshape((n, ))\n",
    "        grad[i] = (F(x + vh) - F(x)) / h\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8e99bf",
   "metadata": {},
   "source": [
    "Let us experimentally evaluate the accuracy of gradient implementations on the set of test functions:\n",
    "\n",
    " - $ f(x) = x^3 + 2x^2 + 12x + 100 \\implies f'(x) = 3x^2 + 4x + 12, f'(2.0) = 32.0 $\n",
    " - $ F(x, y) = x^2 + xy + y^2 \\implies \\begin{cases} \\frac{\\partial F(x, y)}{\\partial x} = 2x + y, \\frac{\\partial F(2.0, -1.0)}{\\partial x} = 3.0 \\\\ \\frac{\\partial F(x, y)}{\\partial y} = x + 2y, \\frac{\\partial F(2.0, -1.0)}{\\partial y} = 0.0\n",
    " \\end{cases} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5edae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 1e-6\n",
    "x_onedim = np.array([2.0])\n",
    "x_muldim = np.array([2.0, -1.0])\n",
    "\n",
    "f = lambda x: x ** 3 + 2 * x ** 2 + 12 * x + 100\n",
    "F = lambda x: x[0] ** 2 + x[0] * x[1] + x[1] ** 2\n",
    "\n",
    "np.testing.assert_almost_equal(lgrad(f, x_onedim, h=h), 32.0, decimal=5)\n",
    "np.testing.assert_almost_equal(rgrad(f, x_onedim, h=h), 32.0, decimal=5)\n",
    "\n",
    "np.testing.assert_almost_equal(lgrad(F, x_muldim, h=h), np.array([3.0, 0.0]), decimal=5)\n",
    "np.testing.assert_almost_equal(rgrad(F, x_muldim, h=h), np.array([3.0, 0.0]), decimal=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b249f373",
   "metadata": {},
   "source": [
    "The accuracy of the approximation depends on the number of nodes on the numerical partitioning grid, thus the smaller step difference, the higher precision. Using the Runge-Romberg-Richardson algorithm [4] we can achieve an increase in the order of the precision of the partitioning grid up to $ O(h^2) $ without adding extra iterations to the approximation algorithm. The approach is based on empirical estimation of the target function value $ z $ while decreasing the step size $ h $. The estimation is based on the assumption:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "z \\approx \\zeta(h) + O(h^p) & (11)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "Let us define a new partitioning grid with the step value of $ r \\cdot h $ and then calculate the value of a target function $ z $ on the grid:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "z \\approx \\zeta(r \\cdot h) + r^p \\cdot O(h^p) & (12)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "By combining two partition grids from (11) and (12), we rearrange terms and discard the main term of the precision order $ O(h^p) $ and get an estimation of the target function value $ z $ with the higher precision order:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "z \\approx \\frac{r^p \\zeta(h) - \\zeta(r \\cdot h)}{r^p - 1} + O(h^{p+1}) & (13)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "Now if we can derive an estimation for the definition in (11) by extracting from it the equation (13):\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "O(h^p) \\approx \\frac{\\zeta(h) - \\zeta(r \\cdot h)}{r^p - 1} & (14)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "By applying the estimations (13) - (14) we get the finite-difference approximation equations for left-side, central and right-side derivatives with the precision order $ O(h^2) $:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\begin{cases} f'(x) = \\frac{-3f(x) + 4f(x + h) - f(x + 2h)}{2h} + \\frac{h^{2} f'''(\\xi)}{3} \\\\ f'(x) = \\frac{f(x + h) - f(x - h)}{2h} + \\frac{h^{2} f'''(\\xi)}{6} \\\\ f'(x) = \\frac{f(x - 2h) - 4f(x - h) + 3f(x)}{2h} + \\frac{h^{2} f'''(\\xi)}{3} \\end{cases} & (15)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "The approach of approximating the gradient value of the target function with a finite-difference schema lets us generalize optimization problems on any kind of analytical functions. Therefore, the convergence of the modern gradient descent algorithms is theoretically proven only for convex and smooth function types. We propose a modification to the existing gradient methods using finite-difference schema that is robust to the non-convex and non-smooth functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a137f2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hlgrad(F: Callable[[np.array], np.array], x: np.array, h=0.001) -> np.array:\n",
    "    n, grad = len(x), np.zeros(x.shape)\n",
    "\n",
    "    for i in range(n):\n",
    "        vh = h * np.eye(1, n, i).reshape((n, ))\n",
    "        grad[i] = (-3.0 * F(x) + 4.0 * F(x + vh) - F(x + 2.0 * vh)) / (2.0 * h)\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "def hcgrad(F: Callable[[np.array], np.array], x: np.array, h=0.001) -> np.array:\n",
    "    n, grad = len(x), np.zeros(x.shape)\n",
    "    \n",
    "    for i in range(n):\n",
    "        vh = h * np.eye(1, n, i).reshape((n, ))\n",
    "        grad[i] = (F(x + vh) - F(x - vh)) / (2.0 * h)\n",
    "    \n",
    "    return grad\n",
    "\n",
    "\n",
    "def hrgrad(F: Callable[[np.array], np.array], x: np.array, h=0.001) -> np.array:\n",
    "    n, grad = len(x), np.zeros(x.shape)\n",
    "\n",
    "    for i in range(n):\n",
    "        vh = h * np.eye(1, n, i).reshape((n, ))\n",
    "        grad[i] = (F(x - 2.0 * vh) - 4.0 * F(x - vh) + 3.0 * F(x)) / (2.0 * h)\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd55aca",
   "metadata": {},
   "source": [
    "Let us experimentally evaluate the accuracy of the same test functions set. We get the same accuracy order for a smaller step size $ h $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9994a846",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 1e-5\n",
    "x_onedim = np.array([2.0])\n",
    "x_muldim = np.array([2.0, -1.0])\n",
    "\n",
    "f = lambda x: x ** 3 + 2 * x ** 2 + 12 * x + 100\n",
    "F = lambda x: x[0] ** 2 + x[0] * x[1] + x[1] ** 2\n",
    "\n",
    "np.testing.assert_almost_equal(hlgrad(f, x_onedim, h=h), 32.0, decimal=5)\n",
    "np.testing.assert_almost_equal(hcgrad(f, x_onedim, h=h), 32.0, decimal=5)\n",
    "np.testing.assert_almost_equal(hrgrad(f, x_onedim, h=h), 32.0, decimal=5)\n",
    "\n",
    "np.testing.assert_almost_equal(hlgrad(F, x_muldim, h=h), np.array([3.0, 0.0]), decimal=5)\n",
    "np.testing.assert_almost_equal(hcgrad(F, x_muldim, h=h), np.array([3.0, 0.0]), decimal=5)\n",
    "np.testing.assert_almost_equal(hrgrad(F, x_muldim, h=h), np.array([3.0, 0.0]), decimal=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
