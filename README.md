# Adaptive Stochastic Gradient Descent

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/antonAce/adaptive-gradient-descent/blob/master/gradient_descent.ipynb)

**The goal of the research:** Implementation and comparison of adaptive stochastic gradient descent methods using Python programming language. The efficiency comparison is demonstrated in the logistic regression optimization problem.

### Attribution

Overview is based on research paper "An overview of gradient descent optimization algorithms" from [arXiv.org](https://arxiv.org/pdf/1609.04747.pdf) by [Sebastian Ruder](mailto:ruder.sebastian@gmail.com) licensed under CC BY-NC-SA 4.0.

### Overviewed gradient algorithms

- Batch gradient descent
- Stochastic gradient descent
- Mini-batch gradient descent
- Momentum
- Nesterov AG
- ADAGRAD
- RMSPROP
- ADAM
