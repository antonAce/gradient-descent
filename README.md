# Investigation of convergence of stochastic gradient and finite-difference optimization methods

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/antonAce/adaptive-gradient-descent/blob/master/notebook/algorithms_overview.ipynb)

## Abstract

Stochastic gradient algorithms have become one of the most popular numerical optimization methods due to their efficiency and formulation simplicity. In general, the convergence rate indicates the performance of the particular stochastic optimization method. The article investigates the inquired metric of stochastic finite-difference optimization methods on convex functions and highlights that it is not an entirely adequate measure of the optimization algorithm's quality in general.

## References

1. Nemirovsky А. and D. Yudin. Informational Complexity and Efficient Methods for Solution of Convex Extremal Problems, J. Wiley & Sons, New York, 1983.
2. Bottou, L., Curtis, F.E., Nocedal, J. (2018). Optimization Methods for Large-Scale Machine Learning. SIAM Review. Vol. 60(2), pp. 223–311. DOI:10.1137/16m1080173
3. The Impact of the Mini-batch Size on the Variance of Gradients in Stochastic Gradient Descent [Text] / Xin Qian, Diego Klabjan.−arXiv Preprint.−Optimization and Control.−2020.−arXiv:2004.13146.
4. Duchi J., Jordan M., Wainwright M., Wibisono A. Optimal rates for zero-order optimization: the power of two function evaluations. [Text] // IEEE Transactions on Information Theory.-2015.-Vol. 61(5).-pp. 2788-2806.
5. Polyak, B.T. (1987). Introduction to Optimization. Optimization Software.
6. Norkin V.I. Two random search algorithms for minimizing non-differentiable functions. Mathematical Methods for Research of Operations and Reliability Theory. [Text] / Yu.M. Ermoliev, I.N. Kovalenko. // Kyiv: Institute of Cybernetics.-1978.-pp. 36–40.
7. A method for unconstrained convex minimization problem with the rate of convergence  [Text] / Yurii Nesterov.-1983.-Reports of the Academy of Sciences.-pp. 543–547.
8. Duchi J., Hazan E., Singer Y. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul.):2121–2159, 2011.
9. Kingma D., Ba J. Adam: a method for stochastic optimization. ICLR, 2015. arXiv:1412.6980v9 [cs.LG] 30 Jan 2017.
